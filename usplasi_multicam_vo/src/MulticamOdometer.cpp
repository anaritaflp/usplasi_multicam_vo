#include <multicam_vo/MulticamOdometer.h>

/** MulticamOdometer default constructor. */
MulticamOdometer::MulticamOdometer()
{
    
}

/** MulticamOdometer constructor.
 * @param Ladybug2 ladybug object
 * @param std::vector<std::ofstream*> vector with files to write estimated poses
 * @return MulticamOdometer object */
MulticamOdometer::MulticamOdometer(Ladybug2 lb2, std::vector<std::ofstream*> files) : node_("~")
{
    files_ = files;
    lb2_ = lb2;
    
	// read odometer parameters
    node_.param<double>("param_odometerInlierThreshold", param_odometerInlierThreshold_, 0.00001);

    monoOdometers_.resize(NUM_OMNI_CAMERAS);
    pubMonoOdoms_.resize(NUM_OMNI_CAMERAS);

    for(int i=0; i<NUM_OMNI_CAMERAS; i++)
    {
        // initialize absolute poses with identities
        absolutePosesLocal_.push_back(Eigen::Matrix4f::Identity());
        firstRow_.push_back(true);

        // initialize monocular odometers
        monoOdometers_[i] = MonoOdometer8(lb2_.cameraMatrices_[i]);

        char topic[100];
        sprintf(topic, "multicam_vo/cam_%d/odometry", i);
        pubMonoOdoms_[i] = node_.advertise<nav_msgs::Odometry>(std::string(topic), 1);
    }
}

/** MulticamOdometer destructor. */
MulticamOdometer::~MulticamOdometer()
{

}

/** Estimate the motion of the multi-camera system.
 * @param std::vector<std::vector<Match>> a vector with each camera's matches
 * @param vector of output matlab files to be filled with the estimated poses
 * @param int& output index of the camera with the most successful motion estimation
 * @param std::vector<std::vector<Match>>& output vector with each camera's inlier matches
 * @param std::vector<std::vector<Eigen::Vector3f>>& output vector with each camera's 3D points triangulated from inlier matches
 * @param std::vector<Eigen::Matrix4f>& output vector with each camera's pose estimation, obtained from monocular VO
 * @return Eigen::Matrix4f transformation with the best relative motion estimate */
Eigen::Matrix4f MulticamOdometer::estimateMotion(std::vector<std::vector<Match>> matches, int &bestCamera, std::vector<std::vector<Match>> &inlierMatches, std::vector<std::vector<Eigen::Vector3f>> &points3D, std::vector<Eigen::Matrix4f> &monoPoses, ros::Time timestamp)
{
    // rotation and translation estimates obtained with the correspondences between each combination of cameras
    std::vector<Eigen::Matrix3f> R;
    std::vector<Eigen::Vector3f> t;
    monoPoses.resize(NUM_OMNI_CAMERAS);
	
	// flags indicating whether R,t was successfully obtained or not
	std::vector<bool> success(NUM_OMNI_CAMERAS, false);
    
    // vectors with the rotation matrices / translation vectors generated by all intra- and consecutive inter-camera matches
	R.resize(NUM_OMNI_CAMERAS);
    t.resize(NUM_OMNI_CAMERAS);

    inlierMatches.resize(NUM_OMNI_CAMERAS);
    points3D.resize(NUM_OMNI_CAMERAS);

	// compute rotation and translation for each set of intra-camera matches
    for(int i=0; i<NUM_OMNI_CAMERAS; i++)
    {
        // get intrinsic matrix K
        Eigen::Matrix3f K(lb2_.cameraMatrices_[i]);

        // estimate monocular visual odometry
        if(i == 0)
        {
            success[i] = monoOdometers_[i].estimateMotion(matches[i],R[i], t[i], false, inlierMatches[i], points3D[i]);
        }
        else
        {
            success[i] = monoOdometers_[i].estimateMotion(matches[i], R[i], t[i], false, inlierMatches[i], points3D[i]);
        }
        monoPoses[i] = Rt2T(R[i], t[i]);   
 
    }

	// let ALL matches vote for the best R,t estimated in the previous mono visual odometry step
    int maxInliers = 0;
    int iMaxInliersInMono = 0;
    int maxInliersInMono = 0;
    bestCamera = 0;
    Eigen::Matrix4f relativePose;
    int maxInlierMatches = 0;
    #pragma omp parallel for
    for(int i=0; i<NUM_OMNI_CAMERAS; i++)
    {
        Eigen::Matrix4f T = Rt2T(R[i], t[i]);
        T = T.inverse();
        Eigen::Matrix4f TGlobal = lb2_.cam2LadybugRef(T, i, i);

        // integrate to absolute mono VO pose estimated by camera i
        absolutePosesLocal_[i] = absolutePosesLocal_[i] * TGlobal;

        // publish individual odometry estimations
        nav_msgs::Odometry msg =  transform2OdometryMsg(absolutePosesLocal_[i], i, inlierMatches[i].size());
        msg.header.stamp = timestamp;
        pubMonoOdoms_[i].publish(msg);

        // if R,t were successfully obtained in camera i...
        if(success[i])
        {
            // vote for motion estimation by camera i
            int sumInliers = 0;

            // test motion estimation by camera i in each camera j
			for(int j=0; j<NUM_OMNI_CAMERAS; j++)
            {
                // transform motion i to camera j coordinates
                Eigen::Matrix4f TLocal = lb2_.Ladybug2CamRef(TGlobal, j, j);
                TLocal = TLocal.inverse();

                // getNumInliers(std::vector<Match> matches, Eigen::Matrix3f R, Eigen::Vector3f t, Eigen::Matrix3f K)
                Eigen::Matrix3f RLocal;
                Eigen::Vector3f tLocal;
                T2Rt(TLocal, RLocal, tLocal);
                                
                ////////////////////// FUNDAMENTAL MATRIX
                Eigen::Matrix3f Fj = Rt2F(RLocal, tLocal, lb2_.cameraMatrices_[j], lb2_.cameraMatrices_[j]);
                std::vector<int> inlierIndices = getInliers(matches[j], Fj, param_odometerInlierThreshold_);
                int numInliers = inlierIndices.size();

                ////////////////////// ESSENTIAL MATRIX
                //int numInliers = getNumInliers(matches[j], RLocal, tLocal, lb2_.cameraMatrices_[j], 1.0);
                //std::cout << i << " " << j << " " << numInliers << std::endl;

                sumInliers += numInliers;
            }
            //std::cout << "inliers in cam " << i << ": " << sumInliers << " max: " << maxInliers << std::endl;

            // the camera i with the most inliers is the best camera
            if((sumInliers > maxInliers) || ((sumInliers == maxInliers) && (inlierMatches[i].size() > inlierMatches[bestCamera].size())))
            //if(inlierMatches[i].size() > maxInlierMatches)
            {
                maxInlierMatches = inlierMatches[i].size();
                maxInliers = sumInliers;
                bestCamera = i;
                relativePose = TGlobal;
            }
        }
    }
    std::cout << "##################### BEST CAM: " << bestCamera << std::endl;

    // inforce translation norm to 1
    Eigen::Matrix3f RFinal;
    Eigen::Vector3f tFinal;
    T2Rt(relativePose, RFinal, tFinal);
    tFinal = tFinal / sqrt(tFinal.dot(tFinal));
    Eigen::Matrix4f TFinal = Rt2T(RFinal, tFinal);

    return TFinal;
}

/** Get the inliers among all matches that comply with a given fundamental matrix.
 * @param std::vector<Match> vector with feature matches
 * @param Eigen::Matrix3f fundamental matrix
 * @param double inlier threshold
 * @return std::vector<int> vector with indices of the inliers */
std::vector<int> MulticamOdometer::getInliers(std::vector<Match> matches, Eigen::Matrix3f F, double threshold)
{
    std::vector<int> inlierIndices;
    
    for(int i=0; i<matches.size(); i++)
    {
        cv::Point2f pPrev = matches[i].pPrev_;
        cv::Point2f pCurr = matches[i].pCurr_;
        Eigen::Vector3f pPrevHomog, pCurrHomog;
        pPrevHomog << pPrev.x, pPrev.y, 1.0;
        pCurrHomog << pCurr.x, pCurr.y, 1.0;

		// xCurr^T * F * xPrev 
        double x2tFx1 = pCurrHomog.transpose() * F * pPrevHomog;

		// F * xPrev
        Eigen::Vector3f Fx1 = F * pPrevHomog;
		
		// F^T * xCurr
        Eigen::Vector3f Ftx2 = F.transpose() * pCurrHomog;
        
		// compute Sampson distance (distance to epipolar line)
        double dSampson = (x2tFx1 * x2tFx1) / ((Fx1(0)*Fx1(0)) + (Fx1(1)*Fx1(1)) + (Ftx2(0)*Ftx2(0)) + (Ftx2(1)*Ftx2(1)));

        if(dSampson < threshold)
        {
            inlierIndices.push_back(i);
        }
    }
    return inlierIndices;
}

/** Compute fundamental martix out of rotation and translation.
 * @param Eigen::Matrix3f rotation matrix
 * @param Eigen::Vector3f translation vector
 * @param Eigen::Matrix3f intrinsics matrix
 * @return Eigen::Matrix3f fundamental matrix */
Eigen::Matrix3f MulticamOdometer::Rt2F(Eigen::Matrix3f R, Eigen::Vector3f t, Eigen::Matrix3f KPrev, Eigen::Matrix3f KCurr)
{
    // get skew symmetric matrix of translation vector
    Eigen::Matrix3f S;
    S << 0.0, -t(2), t(1), t(2), 0.0, -t(0), -t(1), t(0), 0.0;

    // testar R*S
    Eigen::Matrix3f E = S * R;

    Eigen::Matrix3f F = (KCurr.transpose()).inverse() * E * KPrev.inverse();

    // re-enforce rank 2 constraint on fundamental matrix
    Eigen::JacobiSVD<Eigen::Matrix3f> svdF(F, Eigen::ComputeFullU | Eigen::ComputeFullV);
    Eigen::Matrix3f D(Eigen::Matrix3f::Zero());
    D(0, 0) = svdF.singularValues()(0);
    D(1, 1) = svdF.singularValues()(1);
    
    F = (svdF.matrixU()) * D * (svdF.matrixV()).transpose();
    
    return F;
}

int MulticamOdometer::getNumInliers(std::vector<Match> matches, Eigen::Matrix3f rotation, Eigen::Vector3f translation, Eigen::Matrix3f K, double threshold)
{
    if(matches.size() == 0)
    {
        return 0;
    }


    ///////////////////////////////////////////////////
    // 1st try: findInliers()
    Eigen::Matrix3f E, tx;
    tx << 0,               -translation(2), translation(1), 
          translation(2),  0,               -translation(0), 
          -translation(1), translation(0),  0; 
    E = rotation * tx;
    cv::Mat Ecv = (cv::Mat_<double>(3,3) << E(0,0), E(0,1), E(0,2),
                                            E(1,0), E(1,1), E(1,2),
                                            E(2,0), E(2,1), E(2,2));

    std::vector<cv::Point2f> pointsPrev = getPrevPoints(matches);
    std::vector<cv::Point2f> pointsCurr = getCurrPoints(matches);

    cv::Mat m1, m2;
    cv::InputArray pointsPrevCv = (cv::InputArray)pointsPrev;
    cv::InputArray pointsCurrCv = (cv::InputArray)pointsCurr;
    cv::Mat X1 = pointsPrevCv.getMat();
    cv::Mat X2 = pointsCurrCv.getMat();

    const cv::Point2d* x1ptr = X1.ptr<cv::Point2d>();
    const cv::Point2d* x2ptr = X2.ptr<cv::Point2d>();
    int n = X1.checkVector(2);
    cv::Matx33d Ecv2(Ecv.ptr<double>());

    float t = (float)(threshold*threshold);
    int nInliers = 0;
    for (int i = 0; i < n; i++)
    {
        cv::Vec3d x1(x1ptr[i].x, x1ptr[i].y, 1.0);
        cv::Vec3d x2(x2ptr[i].x, x2ptr[i].y, 1.0);
        cv::Vec3d Ex1 = Ecv2 * x1;
        cv::Vec3d Etx2 = Ecv2.t() * x2;
        double x2tEx1 = x2.dot(Ex1);

        double a = Ex1[0] * Ex1[0];
        double b = Ex1[1] * Ex1[1];
        double c = Etx2[0] * Etx2[0];
        double d = Etx2[1] * Etx2[1];

        double error = (float)(x2tEx1 * x2tEx1 / (a + b + c + d));
        if(error < t)
        {
            nInliers++;
        }
    }

   	return nInliers;
    
    /*cv::Mat err, mask, model = Ecv;
	const cv::Point3f* from = m1.ptr<cv::Point3f>();
	const cv::Point3f* to   = m2.ptr<cv::Point3f>();
	const double* F = model.ptr<double>();

    std::cout << "E: " << std::endl << model << std::endl;
    for(int i=0; i<12; i++)
        std::cout << "F(" << i << "): " << F[i] << std::endl;

	//int count = m1.checkVector(3);
    int count = matches.size();

    cv::Mat _err;
    _err.create(count, 1, CV_32F);
    float* errptr = _err.ptr<float>();

    for(int i = 0; i < count; i++ )
    {
        const cv::Point3f& f = from[i];
        const cv::Point3f& t = to[i];

        double a = F[0]*f.x + F[1]*f.y + F[ 2]*f.z + F[ 3] - t.x;
        double b = F[4]*f.x + F[5]*f.y + F[ 6]*f.z + F[ 7] - t.y;
        double c = F[8]*f.x + F[9]*f.y + F[10]*f.z + F[11] - t.z;

        errptr[i] = (float)(a*a + b*b + c*c);
    }
    
    mask.create(_err.size(), CV_8U);
    CV_Assert( _err.isContinuous() && _err.type() == CV_32F && mask.isContinuous() && mask.type() == CV_8U);

    uchar* maskptr = mask.ptr<uchar>();
    float t = (float)(threshold*threshold);
    int i, n = (int)_err.total(), nz = 0;
    for( i = 0; i < n; i++ )
    {
        int f = errptr[i] <= t;
        maskptr[i] = (uchar)f;
        nz += f;
    }

   	return nz;*/

    ///////////////////////////////////////////////////
    // 2nd try: chierality check
    /*cv::Mat RCv = (cv::Mat_<double>(3,3) << rotation(0,0), rotation(0,1), rotation(0,2), 
                                            rotation(1,0), rotation(1,1), rotation(1,2), 
                                            rotation(2,0), rotation(2,1), rotation(2,2));
    cv::Mat tCv = (cv::Mat_<double>(3,1) << translation(0), translation(1), translation(2));
    cv::Mat P0 = cv::Mat::eye(3, 4, RCv.type());
    cv::Mat P1(3, 4, RCv.type());
    P1(cv::Range::all(), cv::Range(0, 3)) = RCv * 1.0; P1.col(3) = tCv * 1.0;

    double dist = 50.0;
    cv::Mat Q;
    cv::triangulatePoints(P0, P1, m1, m2, Q);

    int n = 0;
    for(int i=0; i<Q.cols; i++)
    {
        double x, y, z, w;
        x = Q.at<double>(0, i);
        y = Q.at<double>(1, i);
        z = Q.at<double>(2, i);
        w = Q.at<double>(3, i);
        if(z/w < dist && z/w > 0)
        {
            n++;
        }
    }
    return n;*/
}

int MulticamOdometer::checkChierality(std::vector<Match> matches, Eigen::Matrix3f rotation, Eigen::Vector3f translation)
{
    if(matches.size() == 0)
    {
        return 0;
    }

    Eigen::Matrix3f E, tx;
    tx << 0,               -translation(2), translation(1), 
          translation(2),  0,               -translation(0), 
          -translation(1), translation(0),  0; 
    E = rotation * tx;
    cv::Mat Ecv = (cv::Mat_<double>(3,3) << E(0,0), E(0,1), E(0,2),
                                            E(1,0), E(1,1), E(1,2),
                                            E(2,0), E(2,1), E(2,2));

    std::vector<cv::Point2f> pointsPrev = getPrevPoints(matches);
    std::vector<cv::Point2f> pointsCurr = getCurrPoints(matches);

    cv::Mat m1, m2;
    cv::InputArray pointsPrevCv = (cv::InputArray)pointsPrev;
    cv::InputArray pointsCurrCv = (cv::InputArray)pointsCurr;
    pointsPrevCv.getMat().convertTo(m1, CV_64F);
    pointsCurrCv.getMat().convertTo(m2, CV_64F);

    cv::Mat RCv = (cv::Mat_<double>(3,3) << rotation(0,0), rotation(0,1), rotation(0,2), 
                                            rotation(1,0), rotation(1,1), rotation(1,2), 
                                            rotation(2,0), rotation(2,1), rotation(2,2));
    cv::Mat tCv = (cv::Mat_<double>(3,1) << translation(0), translation(1), translation(2));
    cv::Mat P0 = cv::Mat::eye(3, 4, RCv.type());
    cv::Mat P1(3, 4, RCv.type());
    P1(cv::Range::all(), cv::Range(0, 3)) = RCv * 1.0; P1.col(3) = tCv * 1.0;

    double dist = 50.0;
    cv::Mat Q;
    cv::triangulatePoints(P0, P1, m1, m2, Q);

    int n = 0;
    for(int i=0; i<Q.cols; i++)
    {
        double x, y, z, w;
        x = Q.at<double>(0, i);
        y = Q.at<double>(1, i);
        z = Q.at<double>(2, i);
        w = Q.at<double>(3, i);
        if(z/w < dist && z/w > 0)
        {
            n++;
        }
    }
    return n;
}